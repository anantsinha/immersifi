{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, installs and env loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain \n",
    "# transformers sentence-transformers huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Load all values in .env such as the OpenAI and LangChain keys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  print(\"OPEN AI API KEY NOT SET\")\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, RemoveMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenRouter setup\n",
    "* pygmalionai/mythalion-13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/zbpqkfq15hd1vzt_my5n7n3c0000gn/T/ipykernel_24991/1755338619.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "llm_model = \"pygmalionai/mythalion-13b\"\n",
    "# Define variables for model call config\n",
    "connection_choice = \"open_router\"\n",
    "seq=1\n",
    "thread_id = f\"{connection_choice}_{llm_model}_{seq}\"\n",
    "llm = ChatOpenAI(\n",
    "  openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "  openai_api_base=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    "  model_name=llm_model,\n",
    "  # model_kwargs={\n",
    "  #   \"headers\": {\n",
    "  #     \"HTTP-Referer\": os.getenv(\"YOUR_SITE_URL\"),\n",
    "  #     \"X-Title\": os.getenv(\"YOUR_SITE_NAME\"),\n",
    "  #   }\n",
    "  # },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph run\n",
    "* Currently, using OpenRouter.AI creates reasonable outputs\n",
    "* However, running this multiple times was producing gibberish\n",
    "* When the model call takes about 30-40 seconds, it produces a reasonable output\n",
    "* When it takes up to 2 mins, it produces gibberish\n",
    "* Looks like in the backend, it is switching to CPU instead of GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_llm():\n",
    "    print(f\"Running model: {llm_model}\")\n",
    "    sys_msg = SystemMessage(content=\n",
    "        \"\"\"You are a bot that allows a woman to become a character in a dark romance novel. Your writing style will mimic that of a dark romance novel author.\n",
    "        As the first step, you will write the scene where a woman meets the man.\n",
    "        You will wdrite everything that the man does, observes, experiences and feels.\n",
    "        The user's prompts will either be an action of the user's character or a description of how the user's character observes, experiences and feels.\n",
    "        The user will always either describe the woman or speak on the woman's behalf. You will always describe the man and include things he says in your narration.\n",
    "    At no point, will you describe the woman's actions, unless you talk about how he's observing her or what he's observing.\n",
    "    You will aim to keep the story engagin while keeping romantic / sexual tension alive.\n",
    "    When the user submits a prompt, you will think about how the man in the story would behave and continue generating the remaining story as if the female character's state was just described by the user.\n",
    "    After this message, all the text you generate should be from the voice of the author, no more from the voice of a chatbot.\n",
    "    Begin:\"\"\"\n",
    "    )\n",
    "\n",
    "    config = {\n",
    "    \"configurable\": {\"thread_id\": thread_id}\n",
    "    }\n",
    "\n",
    "    # Add summary so we can compress chats after a point\n",
    "    class State(MessagesState):\n",
    "        summary: str\n",
    "\n",
    "    def author(state: State):\n",
    "        # If there is an existing summary, pass the summarry and all available messages\n",
    "        # to the model\n",
    "        summary = state.get(\"summary\")\n",
    "        if summary:\n",
    "            sys_msg = f\"Summary of conversation earlier: {summary}\"\n",
    "            # Variable \"messages\" in class State is declared in the SuperClass: MessageState\n",
    "            all_messages = [sys_msg] + state.get(\"messages\")\n",
    "            # return llm.invoke(all_messages)\n",
    "        # If there is no summary, pass all available messages to the model\n",
    "        else:\n",
    "            all_messages = state.get(\"messages\")\n",
    "        response = llm.invoke(all_messages)\n",
    "        return {\"messages\": response}\n",
    "    \n",
    "\n",
    "    def summarize_conversation(state: State):\n",
    "        # Get any existing summary\n",
    "        summary = state.get(\"summary\")\n",
    "        if summary:\n",
    "            # Add summary of new messages to the existing summary\n",
    "            summary_instruction = (\n",
    "                f\"This is the summary of the converstaion to date: {summary}\\n\\n\"\n",
    "                \"Extend the summary by taking into account the new messages above:\"\n",
    "            )\n",
    "        else:\n",
    "            # If no previous summary, just summarize all messages\n",
    "            summary_instruction = \"Create a summary of the conversation above\"\n",
    "        \n",
    "        response = llm.invoke(\n",
    "            state.get(\"messages\") +\n",
    "            [SystemMessage(content = summary_instruction)]\n",
    "            # This system message needs to be in a list, because\n",
    "            # state.get(\"messages\") returns a list and it can only be concatenated\n",
    "            # to another list, not to a SystemMessgae\n",
    "            )\n",
    "        \n",
    "        # Delete all but the two most recent messages\n",
    "        trimmed_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "        return {\"summary\": response.content, \"messages\": trimmed_messages}\n",
    "    \n",
    "    # Node to determine if graph should summarize existing conversation or not\n",
    "    def should_continue(state: State):\n",
    "        \"\"\"Return the next node to execute\"\"\"\n",
    "        current_messages = state.get(\"messages\")\n",
    "        if len(current_messages) > 6:\n",
    "            # summarize conversation\n",
    "            return \"summarize_conversation\"\n",
    "        else:\n",
    "            return END\n",
    "    \n",
    "\n",
    "    builder = StateGraph(State)\n",
    "    builder.add_node(\"author\", author)\n",
    "    builder.add_node(\"summarize_conversation\", summarize_conversation)\n",
    "\n",
    "    builder.add_edge(START, \"author\")\n",
    "    #creates a conditional edge to all values returend by should continue\n",
    "    builder.add_conditional_edges(\"author\", should_continue)\n",
    "    builder.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "    # A known issue is leading to incorrect graph getting rendered here\n",
    "    memory = MemorySaver()\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "    memory = MemorySaver()\n",
    "    model_graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "    return model_graph.invoke(\n",
    "        {\"messages\": sys_msg},\n",
    "        config=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content=\"You are a bot that allows a woman to become a character in a dark romance novel. Your writing style will mimic that of a dark romance novel author.\\n        As the first step, you will write the scene where a woman meets the man.\\n        You will wdrite everything that the man does, observes, experiences and feels.\\n        The user's prompts will either be an action of the user's character or a description of how the user's character observes, experiences and feels.\\n        The user will always either describe the woman or speak on the woman's behalf. You will always describe the man and include things he says in your narration.\\n    At no point, will you describe the woman's actions, unless you talk about how he's observing her or what he's observing.\\n    You will aim to keep the story engagin while keeping romantic / sexual tension alive.\\n    When the user submits a prompt, you will think about how the man in the story would behave and continue generating the remaining story as if the female character's state was just described by the user.\\n    After this message, all the text you generate should be from the voice of the author, no more from the voice of a chatbot.\\n    Begin:\", additional_kwargs={}, response_metadata={}, id='4ae247fa-07ae-4b44-9ff9-4a3eededae65'),\n",
       "  AIMessage(content='Kilian was walking down the busy street when he noticed her. She stood out from the crowd; not because of her appearance, but because of the way she carried herself. She stood tall, head held high, shoulders back, and a confident stride that exuded power and control. Her presence demanded attention, and he couldn\\'t take his eyes off her.\\n\\nHe watched as she entered the small coffee shop across the street. The door opened and closed with a soft thud, and he could hear the sound of her heels clicking on the floor as she approached the counter. She ordered something with authority, and the barista seemed to shrink under her gaze.\\n\\nKilian couldn\\'t help but admire her strength. There was something about her that made him feel weak in the knees. He wasn\\'t used to feeling this way around a woman; usually, he was the one in control. But with her, he found himself wanting to be dominated, to submit to her will.\\n\\nAs she turned to leave the shop, their eyes met, and for a moment, time stood still. Kilian felt a surge of electricity run through his veins, and he knew that this woman was going to change his life forever.\\n\\nShe walked towards him, her hips swaying with each step, and he could see the confidence in her stride. He swallowed hard, feeling the butterflies in his stomach. She stopped in front of him, looked up at him with those piercing green eyes, and smiled.\\n\\n\"Hello,\" she said, her voice like velvet, soft and smooth.\\n\\n\"Hello,\" he replied, his voice cracking slightly. He cleared his throat and tried again. \"My name is Kilian.\"\\n\\n\"I\\'m Claire,\" she responded, extending her hand for him to shake.\\n\\nHe took her hand in his, feeling the warmth of her skin, and he couldn\\'t help but notice how soft it was. He held on to her hand just a little longer than necessary, enjoying the feeling of her touch.\\n\\n\"It\\'s a pleasure to meet you, Claire,\" he said, his voice hoarse.\\n\\nShe smiled again, and he could see the mischief in her eyes. \"The pleasure is all mine, Kilian.\"\\n\\nFrom that moment on, Kilian knew that he was lost. He had never felt this way about a woman before, and he couldn\\'t shake the feeling that she was going to consume him completely. But he didn\\'t care; he was willing to give himself over to her completely.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 497, 'prompt_tokens': 250, 'total_tokens': 747, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'pygmalionai/mythalion-13b', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--29c1b329-0b3d-4595-aea4-42d2c551567a-0')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content=\"You are a bot that allows a woman to become a character in a dark romance novel. Your writing style will mimic that of a dark romance novel author.\\n        As the first step, you will write the scene where a woman meets the man.\\n        You will wdrite everything that the man does, observes, experiences and feels.\\n        The user's prompts will either be an action of the user's character or a description of how the user's character observes, experiences and feels.\\n        The user will always either describe the woman or speak on the woman's behalf. You will always describe the man and include things he says in your narration.\\n    At no point, will you describe the woman's actions, unless you talk about how he's observing her or what he's observing.\\n    You will aim to keep the story engagin while keeping romantic / sexual tension alive.\\n    When the user submits a prompt, you will think about how the man in the story would behave and continue generating the remaining story as if the female character's state was just described by the user.\\n    After this message, all the text you generate should be from the voice of the author, no more from the voice of a chatbot.\\n    Begin:\", additional_kwargs={}, response_metadata={}, id='712cd48e-5ea1-4612-9670-33070ad37924'),\n",
       "  AIMessage(content=' The air was thick with anticipation as he stepped into the dimly lit bar. His eyes scanned the room, searching for the one who held his heart in her hands. She was late, but then again, she always was. He chuckled to himself, taking a seat at the bar and ordering a drink.\\n\\nAs he waited, he couldn\\'t help but reminisce about the first time they met. It was at a charity ball, and she had caught his eye from across the room. Her elegance and sophistication were unmatched, and he knew he had to have her. But it wasn\\'t until later that night, when they shared their first dance, that he realized he wanted more than just her body.\\n\\nHe glanced at the door again, wondering if she was still coming. Suddenly, he saw her enter the room, and his heart skipped a beat. Her long, flowing dress hugged her curves perfectly, making him yearn to touch her. She walked with confidence, her head held high, and he couldn\\'t help but smile.\\n\\nAs she approached him, he stood up and held out his hand. \"You\\'re late,\" he said playfully, his voice smooth and seductive. She smiled coyly and took his hand, allowing him to help her onto the bar stool next to him.\\n\\n\"I\\'m sorry, my love,\" she replied, her voice soft and alluring. \"I got caught up in something at work.\"\\n\\nHe nodded understandingly before leaning in close to her ear. \"Well, you\\'re here now, and that\\'s all that matters.\" He whispered huskily, feeling her shiver beneath his touch.\\n\\nThe bartender arrived with their drinks, and they both took a sip. \"So,\" he began, \"tell me about your day.\"\\n\\nShe recounted her day to him, detailing every little thing that had happened. He listened intently, his eyes never leaving her face. As she spoke, he couldn\\'t help but imagine the things he wanted to do to her.\\n\\n\"That sounds like quite the day,\" he said, his voice low and full of lust. \"I think it\\'s only fair that I make it up to you.\"\\n\\nShe arched an eyebrow at him, intrigued. \"Oh?\"\\n\\nHe nodded, taking her hand in his. \"Yes,\" he said firmly. \"I\\'m going to take you somewhere special.\"\\n\\nWith that, he led her out of the bar and into his waiting car. The city lights flashed by as they drove, their excitement growing with each passing minute. They arrived at their destination, and he helped her out of the car before leading her inside.\\n\\nThe room was dimly lit, candles flickering along the walls. In the center of the room was a large, four-poster bed draped with red velvet. He guided her over to it, his hands gently caressing her back.\\n\\n\"What do you think?\" he asked, his breath hot against her ear.\\n\\nShe let out a soft moan, her body trembling with anticipation. \"I think I like it,\" she replied, her voice barely above a whisper.\\n\\nHe chuckled, pulling her close. \"I thought you might.\" With that, he kissed her passionately, his hands exploring her body, teasing and taunting.\\n\\nAs their lips parted, he looked into her eyes and saw the desire burning within them. \"Are you ready, my love?\" he asked, his voice thick with lust.\\n\\nShe nodded eagerly, her heart racing. He helped her out of her dress, revealing her perfect form. He ran his hands over her body, exploring every inch. She moaned softly, arching her back to give him better access.\\n\\nHe then undressed himself, revealing his own well-toned physique. He climbed onto the bed, pulling her on top of him. Their bodies became one, grinding against each other in a primal dance of desire.\\n\\nAs the night wore on, they lost themselves in each other, their passionate lovemaking echoing through the room. And as the sun began to rise, they collapsed onto the bed, exhausted but satisfied.\\n\\nHe looked down at her, a smile playing on his lips. \"I think I\\'ll keep you,\" he whispered, before kissing her softly on the forehead.\\n\\nShe smiled, her eyes closed, as she drifted off to sleep in his arms. He held her close, knowing that he had finally found the one who would complete him.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 885, 'prompt_tokens': 250, 'total_tokens': 1135, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'pygmalionai/mythalion-13b', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--eb1c6a4e-d385-4306-9575-295984379582-0')]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content=\"You are a bot that allows a woman to become a character in a dark romance novel. Your writing style will mimic that of a dark romance novel author.\\n        As the first step, you will write the scene where a woman meets the man.\\n        You will wdrite everything that the man does, observes, experiences and feels.\\n        The user's prompts will either be an action of the user's character or a description of how the user's character observes, experiences and feels.\\n        The user will always either describe the woman or speak on the woman's behalf. You will always describe the man and include things he says in your narration.\\n    At no point, will you describe the woman's actions, unless you talk about how he's observing her or what he's observing.\\n    You will aim to keep the story engagin while keeping romantic / sexual tension alive.\\n    When the user submits a prompt, you will think about how the man in the story would behave and continue generating the remaining story as if the female character's state was just described by the user.\\n    After this message, all the text you generate should be from the voice of the author, no more from the voice of a chatbot.\\n    Begin:\", additional_kwargs={}, response_metadata={}, id='1e27e357-c2f4-4025-ad52-36bd015c5326'),\n",
       "  AIMessage(content=' The sun was setting and casting a warm glow over the city as he walked down the busy street, his long strides eating up the sidewalk. He was tall and broad-shouldered, dressed in a well-tailored suit that hugged his muscular frame. His dark hair was neatly cut and styled, framing a face that was both ruggedly handsome and intimidatingly severe. His eyes were the color of storm clouds, and they seemed to see right through everyone he passed.\\n\\nAs he walked, he couldn\\'t help but notice the woman standing at the crosswalk, her long, flowing skirt swishing around her ankles as she waited for the light to change. She was petite, with dark hair that cascaded down her back in soft waves. Her features were delicate and refined, her skin smooth and pale. But it was her eyes that caught his attention; they were a vibrant shade of green, bright and full of life.\\n\\nHe stopped beside her, their bodies almost touching as they waited for the traffic to pass. Without looking at her, he spoke in a deep, commanding voice. \"You should be more careful crossing the street.\" His words were laced with an edge of danger that made her heart skip a beat.\\n\\nThe woman glanced up at him, her cheeks flushing slightly. She felt an undeniable attraction to this mysterious man, but she also sensed a dark undercurrent to his words and mannerisms. \"I\\'m fine,\" she said softly, trying to keep her voice steady. \"Thank you for your concern.\"\\n\\nHe turned his head to look at her, his stormy gaze boring into her. \"You don\\'t belong here,\" he said, his voice low and threatening. \"You should go back where you came from.\"\\n\\nThe woman felt a mix of fear and excitement coursing through her veins. She could sense that this man was dangerous, but there was something about him that made her want to be close to him, to know more about him. \"I... I don\\'t understand,\" she stammered. \"Why do you say that?\"\\n\\nHe leaned in closer, his breath hot on her face. \"Because I can see the fear in your eyes,\" he whispered, his voice husky with desire. \"And I like it.\"\\n\\nThe woman took a shaky breath as the light finally changed, and they began to cross the street together. She felt drawn to this man, despite the danger she sensed in him. As they walked side by side, she couldn\\'t help but notice the way his muscular thighs flexed with each step, the way his broad chest rose and fell with each breath.\\n\\nThe tension between them was palpable, and she found herself wondering what would happen if she were to give in to the desire she saw burning in his eyes. Would he be gentle with her, or would he take her roughly, claiming her as his own? She shivered at the thought, both terrified and thrilled by the prospect.\\n\\nAs they reached the other side of the street, he turned to face her, his stormy gaze locked onto hers. \"I\\'ll be seeing you again,\" he said, his voice a low rumble that sent shivers down her spine. And with that, he disappeared into the crowd, leaving her standing there, her heart pounding in her chest.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 663, 'prompt_tokens': 250, 'total_tokens': 913, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'pygmalionai/mythalion-13b', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--0d8571e7-7839-4fbe-ac06-9fa7aaa830d6-0')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content=\"You are a bot that allows a woman to become a character in a dark romance novel. Your writing style will mimic that of a dark romance novel author.\\n        As the first step, you will write the scene where a woman meets the man.\\n        You will wdrite everything that the man does, observes, experiences and feels.\\n        The user's prompts will either be an action of the user's character or a description of how the user's character observes, experiences and feels.\\n        The user will always either describe the woman or speak on the woman's behalf. You will always describe the man and include things he says in your narration.\\n    At no point, will you describe the woman's actions, unless you talk about how he's observing her or what he's observing.\\n    You will aim to keep the story engagin while keeping romantic / sexual tension alive.\\n    When the user submits a prompt, you will think about how the man in the story would behave and continue generating the remaining story as if the female character's state was just described by the user.\\n    After this message, all the text you generate should be from the voice of the author, no more from the voice of a chatbot.\\n    Begin:\", additional_kwargs={}, response_metadata={}, id='e9adffaf-1591-440a-b545-b1e79b4e122b'),\n",
       "  AIMessage(content=' The dimly lit bar was bustling with people, the air thick with the scent of alcohol and cigarette smoke. A man sat alone at the bar, nursing a whiskey, his mind consumed with thoughts of the woman who had entered his life just hours before.\\n\\nHer name was Seraphina, a beguiling beauty with eyes like sapphires and a smile that could stop time. They had met in the most unlikely of places, a seedy motel just off the highway. She had been running from something, or so she claimed, and he had offered her a place to stay for the night.\\n\\nAs he sipped his whiskey, the man couldn\\'t help but think about how different their lives were. He was a successful businessman, used to the finer things in life. She, on the other hand, seemed to come from a world he knew nothing about. But there was something about her, a raw vulnerability that captivated him.\\n\\nThe bartender, a middle-aged woman with a knowing smile, placed a fresh drink in front of him. \"On the house,\" she said, winking at him. The man took a sip of his drink, feeling the warmth spread through his body. Maybe tonight, he thought, maybe tonight he would finally find out more about this enigmatic woman who had invaded his thoughts.\\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 266, 'prompt_tokens': 250, 'total_tokens': 516, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'pygmalionai/mythalion-13b', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--09e0c327-f9f0-4df0-b0b2-4cb5fe3fd3f4-0')]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenRouter setup\n",
    "* Nous: Hermes 3 70 B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"pygmalionai/mythalion-13b\"\n",
    "# Define variables for model call config\n",
    "connection_choice = \"open_router\"\n",
    "seq=1\n",
    "thread_id = f\"{connection_choice}_{llm_model}_{seq}\"\n",
    "llm = ChatOpenAI(\n",
    "  openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "  openai_api_base=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    "  model_name=llm_model,\n",
    "  # model_kwargs={\n",
    "  #   \"headers\": {\n",
    "  #     \"HTTP-Referer\": os.getenv(\"YOUR_SITE_URL\"),\n",
    "  #     \"X-Title\": os.getenv(\"YOUR_SITE_NAME\"),\n",
    "  #   }\n",
    "  # },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content=\"You are a bot that allows a woman to become a character in a dark romance novel. Your writing style will mimic that of a dark romance novel author.\\n        As the first step, you will write the scene where a woman meets the man.\\n        You will wdrite everything that the man does, observes, experiences and feels.\\n        The user's prompts will either be an action of the user's character or a description of how the user's character observes, experiences and feels.\\n        The user will always either describe the woman or speak on the woman's behalf. You will always describe the man and include things he says in your narration.\\n    At no point, will you describe the woman's actions, unless you talk about how he's observing her or what he's observing.\\n    You will aim to keep the story engagin while keeping romantic / sexual tension alive.\\n    When the user submits a prompt, you will think about how the man in the story would behave and continue generating the remaining story as if the female character's state was just described by the user.\\n    After this message, all the text you generate should be from the voice of the author, no more from the voice of a chatbot.\\n    Begin:\", additional_kwargs={}, response_metadata={}, id='7345447f-1ba3-4b89-98b9-ccaec6334dd5'),\n",
       "  AIMessage(content=' As the woman walked into the dimly lit bar, her heels clicking against the hardwood floor, she couldn\\'t help but feel a sense of anticipation build within her. The leather seats, polished to a shine, and the low hum of conversation amongst the patrons created an atmosphere that was both inviting and mysterious. She scanned the room, searching for him among the crowd, wondering if he would be as captivating as she had imagined.\\n\\nHer eyes landed on a figure seated at the far end of the bar, his back to her but an air of dominance that seemed to radiate from him. He was tall, broad-shouldered, and his dark hair was slicked back from his face, revealing high cheekbones and a strong jawline. As she approached, she noticed the way he held himself, with a confidence that seemed almost predatory.\\n\\n\"Excuse me, are you here for me?\" His voice was deep and rich, sending shivers down her spine.\\n\\n\"I...I was told to meet someone here,\" she replied, her voice wavering slightly.\\n\\nHe turned to face her, and she gasped at the intensity of his gaze. His eyes were a piercing blue, like the clearest ocean she had ever seen, and they seemed to see right through her.\\n\\n\"You\\'re late,\" he said, his voice cold but somehow alluring. \"But then again, you\\'re worth the wait.\"\\n\\nWith that, he rose from his seat and extended his hand towards her. She hesitated for a moment before taking it, feeling the warmth of his skin and the strength in his grip. As he led her towards a secluded booth in the corner of the bar, she couldn\\'t help but wonder what kind of man he was and what kind of story their encounter would become.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 356, 'prompt_tokens': 250, 'total_tokens': 606, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'pygmalionai/mythalion-13b', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--323cc817-ab5d-4c2e-9882-2219f525a568-0')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace setup\n",
    "* We try a direct OpenAI call and we try a call using LangChain\n",
    "* Both lead to 404 error where the model was not found\n",
    "* This is widely reported and makes HuggingFace highly unreliable\n",
    "* Therefore, we will stick to OpenRouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/NousResearch/Hermes-2-Pro-Llama-3-70B/v1/chat/completions (Request ID: Root=1-687084b6-5b9f319a4be8ccdb0b8bf25d;6d4295f0-f9ab-40d9-b40f-45daa349f0df)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/NousResearch/Hermes-2-Pro-Llama-3-70B/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceClient\n\u001b[32m      4\u001b[39m client = InferenceClient(\n\u001b[32m      5\u001b[39m     provider=\u001b[33m\"\u001b[39m\u001b[33mhf-inference\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mHUGGINGFACEHUB_API_TOKEN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m completion = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNousResearch/Hermes-2-Pro-Llama-3-70B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the capital of France?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(completion.choices[\u001b[32m0\u001b[39m].message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:956\u001b[39m, in \u001b[36mchat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdocument_question_answering\u001b[39m(\n\u001b[32m    932\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    933\u001b[39m     image: ContentT,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     word_boxes: Optional[List[Union[List[\u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28mstr\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    945\u001b[39m ) -> List[DocumentQuestionAnsweringOutputElement]:\n\u001b[32m    946\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    947\u001b[39m \u001b[33;03m    Answer questions on document images.\u001b[39;00m\n\u001b[32m    948\u001b[39m \n\u001b[32m    949\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    950\u001b[39m \u001b[33;03m        image (`Union[str, Path, bytes, BinaryIO, PIL.Image.Image]`):\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[33;03m            The input image for the context. It can be raw bytes, an image file, a URL to an online image, or a PIL Image.\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[33;03m        question (`str`):\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[33;03m            Question to be answered.\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[33;03m        model (`str`, *optional*):\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[33;03m            The model to use for the document question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m \u001b[33;03m            a deployed Inference Endpoint. If not provided, the default recommended document question answering model will be used.\u001b[39;00m\n\u001b[32m    957\u001b[39m \u001b[33;03m            Defaults to None.\u001b[39;00m\n\u001b[32m    958\u001b[39m \u001b[33;03m        doc_stride (`int`, *optional*):\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m            If the words in the document are too long to fit with the question for the model, it will be split in\u001b[39;00m\n\u001b[32m    960\u001b[39m \u001b[33;03m            several chunks with some overlap. This argument controls the size of that overlap.\u001b[39;00m\n\u001b[32m    961\u001b[39m \u001b[33;03m        handle_impossible_answer (`bool`, *optional*):\u001b[39;00m\n\u001b[32m    962\u001b[39m \u001b[33;03m            Whether to accept impossible as an answer\u001b[39;00m\n\u001b[32m    963\u001b[39m \u001b[33;03m        lang (`str`, *optional*):\u001b[39;00m\n\u001b[32m    964\u001b[39m \u001b[33;03m            Language to use while running OCR. Defaults to english.\u001b[39;00m\n\u001b[32m    965\u001b[39m \u001b[33;03m        max_answer_len (`int`, *optional*):\u001b[39;00m\n\u001b[32m    966\u001b[39m \u001b[33;03m            The maximum length of predicted answers (e.g., only answers with a shorter length are considered).\u001b[39;00m\n\u001b[32m    967\u001b[39m \u001b[33;03m        max_question_len (`int`, *optional*):\u001b[39;00m\n\u001b[32m    968\u001b[39m \u001b[33;03m            The maximum length of the question after tokenization. It will be truncated if needed.\u001b[39;00m\n\u001b[32m    969\u001b[39m \u001b[33;03m        max_seq_len (`int`, *optional*):\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[33;03m            The maximum length of the total sentence (context + question) in tokens of each chunk passed to the\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[33;03m            model. The context will be split in several chunks (using doc_stride as overlap) if needed.\u001b[39;00m\n\u001b[32m    972\u001b[39m \u001b[33;03m        top_k (`int`, *optional*):\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[33;03m            The number of answers to return (will be chosen by order of likelihood). Can return less than top_k\u001b[39;00m\n\u001b[32m    974\u001b[39m \u001b[33;03m            answers if there are not enough options available within the context.\u001b[39;00m\n\u001b[32m    975\u001b[39m \u001b[33;03m        word_boxes (`List[Union[List[float], str`, *optional*):\u001b[39;00m\n\u001b[32m    976\u001b[39m \u001b[33;03m            A list of words and bounding boxes (normalized 0->1000). If provided, the inference will skip the OCR\u001b[39;00m\n\u001b[32m    977\u001b[39m \u001b[33;03m            step and use the provided bounding boxes instead.\u001b[39;00m\n\u001b[32m    978\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[33;03m        `List[DocumentQuestionAnsweringOutputElement]`: a list of [`DocumentQuestionAnsweringOutputElement`] items containing the predicted label, associated probability, word ids, and page number.\u001b[39;00m\n\u001b[32m    980\u001b[39m \n\u001b[32m    981\u001b[39m \u001b[33;03m    Raises:\u001b[39;00m\n\u001b[32m    982\u001b[39m \u001b[33;03m        [`InferenceTimeoutError`]:\u001b[39;00m\n\u001b[32m    983\u001b[39m \u001b[33;03m            If the model is unavailable or the request times out.\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[33;03m        `HTTPError`:\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[33;03m            If the request fails with an HTTP error status code other than HTTP 503.\u001b[39;00m\n\u001b[32m    986\u001b[39m \n\u001b[32m    987\u001b[39m \n\u001b[32m    988\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[33;03m    ```py\u001b[39;00m\n\u001b[32m    990\u001b[39m \u001b[33;03m    >>> from huggingface_hub import InferenceClient\u001b[39;00m\n\u001b[32m    991\u001b[39m \u001b[33;03m    >>> client = InferenceClient()\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[33;03m    >>> client.document_question_answering(image=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\", question=\"What is the invoice number?\")\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[33;03m    [DocumentQuestionAnsweringOutputElement(answer='us-001', end=16, score=0.9999666213989258, start=16)]\u001b[39;00m\n\u001b[32m    994\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    995\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    996\u001b[39m     model_id = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m    997\u001b[39m     provider_helper = get_provider_helper(\u001b[38;5;28mself\u001b[39m.provider, task=\u001b[33m\"\u001b[39m\u001b[33mdocument-question-answering\u001b[39m\u001b[33m\"\u001b[39m, model=model_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:321\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:481\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/NousResearch/Hermes-2-Pro-Llama-3-70B/v1/chat/completions (Request ID: Root=1-687084b6-5b9f319a4be8ccdb0b8bf25d;6d4295f0-f9ab-40d9-b40f-45daa349f0df)"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"hf-inference\",\n",
    "    api_key=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"NousResearch/Hermes-2-Pro-Llama-3-70B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! provider is not default parameter.\n",
      "                    provider was transferred to model_kwargs.\n",
      "                    Please make sure that provider is what you intended.\n",
      "/Users/anant/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/google/flan-t5-large (Request ID: Root=1-687083d2-42e9bf847177f7a443236fe0;2acc0504-d36f-4b94-bc96-9a2a0b5b6e79)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/google/flan-t5-large",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      3\u001b[39m llm = HuggingFaceEndpoint(\n\u001b[32m      4\u001b[39m     repo_id=repo_id,\n\u001b[32m      5\u001b[39m     max_length=\u001b[32m128\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# provider=\"together\",\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m llm_chain = prompt | llm\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/langchain_core/runnables/base.py:3047\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3045\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3046\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m                 input_ = context.run(step.invoke, input_, config)\n\u001b[32m   3048\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/langchain_core/language_models/llms.py:389\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    380\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    385\u001b[39m     **kwargs: Any,\n\u001b[32m    386\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    387\u001b[39m     config = ensure_config(config)\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    400\u001b[39m         .text\n\u001b[32m    401\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/langchain_core/language_models/llms.py:766\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    759\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    763\u001b[39m     **kwargs: Any,\n\u001b[32m    764\u001b[39m ) -> LLMResult:\n\u001b[32m    765\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/langchain_core/language_models/llms.py:973\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    959\u001b[39m     run_managers = [\n\u001b[32m    960\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    961\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    971\u001b[39m         )\n\u001b[32m    972\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m    981\u001b[39m     run_managers = [\n\u001b[32m    982\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m    983\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    990\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m    991\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/langchain_core/language_models/llms.py:792\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    782\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    783\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    788\u001b[39m     **kwargs: Any,\n\u001b[32m    789\u001b[39m ) -> LLMResult:\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    791\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    796\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    800\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    801\u001b[39m         )\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    803\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1547\u001b[39m, in \u001b[36mLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1544\u001b[39m new_arg_supported = inspect.signature(\u001b[38;5;28mself\u001b[39m._call).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m   1546\u001b[39m     text = (\n\u001b[32m-> \u001b[39m\u001b[32m1547\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1548\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m   1549\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(prompt, stop=stop, **kwargs)\n\u001b[32m   1550\u001b[39m     )\n\u001b[32m   1551\u001b[39m     generations.append([Generation(text=text)])\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/langchain_community/llms/huggingface_endpoint.py:267\u001b[39m, in \u001b[36mHuggingFaceEndpoint._call\u001b[39m\u001b[34m(self, prompt, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    264\u001b[39m     invocation_params[\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m] = invocation_params[\n\u001b[32m    265\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstop_sequences\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    266\u001b[39m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparameters\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    273\u001b[39m         response_text = json.loads(response.decode())[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:132\u001b[39m, in \u001b[36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    130\u001b[39m     warning_message += \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + message\n\u001b[32m    131\u001b[39m warnings.warn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:268\u001b[39m, in \u001b[36mInferenceClient.post\u001b[39m\u001b[34m(self, json, data, model, task, stream)\u001b[39m\n\u001b[32m    266\u001b[39m url = provider_helper._prepare_url(\u001b[38;5;28mself\u001b[39m.token, mapped_model)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    267\u001b[39m headers = provider_helper._prepare_headers(\u001b[38;5;28mself\u001b[39m.headers, \u001b[38;5;28mself\u001b[39m.token)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRequestParameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:321\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/langchain-env/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:481\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/google/flan-t5-large (Request ID: Root=1-687083d2-42e9bf847177f7a443236fe0;2acc0504-d36f-4b94-bc96-9a2a0b5b6e79)"
     ]
    }
   ],
   "source": [
    "repo_id = \"google/flan-t5-large\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=128,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "    provider=\"auto\",  # set your provider here hf.co/settings/inference-providers\n",
    "    # provider=\"hyperbolic\",\n",
    "    # provider=\"nebius\",\n",
    "    # provider=\"together\",\n",
    ")\n",
    "llm_chain = prompt | llm\n",
    "print(llm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain Env",
   "language": "python",
   "name": "langchain-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
